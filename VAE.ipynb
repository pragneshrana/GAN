{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOMtUjyo0zDs6FMsKMf3A/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pragneshrana/GAN/blob/master/VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WrV_tVsS0YS"
      },
      "source": [
        "- Autoencoders are closely related to principal component analysis (PCA). \n",
        "- split the network into two segments, the encoder, and the decoder\n",
        "- $\\phi : \\mathcal X \\rightarrow \\mathcal F $\n",
        "- $\\psi : \\mathcal F \\rightarrow \\mathcal X   $\n",
        "- $\\phi , \\psi = argmin_{\\phi,\\psi} ||X - (\\psi o \\phi) X||^2$\n",
        "\n",
        "Basically, trying to recreate the original image after some generalized non-linear compression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFmwcoOrWBpb"
      },
      "source": [
        "- Encoding Network\n",
        "  - $z = \\sigma (Wx+b) $\n",
        "- decoding Network\n",
        "  - $x' = \\sigma' (W'z+b') $\n",
        "\n",
        "- Loss : \n",
        "  $\\mathcal L(x,x') = || x-x'|| = ||x - \\sigma '(W' (\\sigma (Wx+b)) + b')||^2$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzzRql5sWBGP"
      },
      "source": [
        "- This is self-supervised learning.\n",
        "- **aim** of the autoencoder is to select our encoder and decoder functions in such a way that we require the minimal information to encode the image such that it be can regenerated on the other side."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV_vlFPjgmoa"
      },
      "source": [
        "Types of AutoEncoders :\n",
        "- Denoising Autoencoders : add some white noise to the data prior to training also compare the error to the original image when training. This forces the network to **not become overfit to arbitrary noise** present in images. \n",
        "\n",
        "- Sparse Autoencoders : has a larger latent dimension than the input or output dimensions. But only a small fraction of the neurons fires, meaning that the network is inherently ‘sparse’. It forms form regularization to reduce the propensity for the network to overfit.\n",
        "\n",
        "- Contractive Autoencoder: do not alter the architecture and simply add a regularizer to the loss function. This can be thought of as a neural form of ridge regression.\n",
        "\n",
        "- Variatioal Autoencoders : Implemented a form of variational inference taken from Bayesian statistics. It learn a data generating distribution, which allows us to take random samples from the latent space. These random samples can then be decoded using the decoder network to generate unique images that have similar characteristics to those that the network was trained on.\n",
        "\n",
        "Ref: https://jaan.io/what-is-variational-autoencoder-vae-tutorial/\n",
        "\n",
        "https://towardsdatascience.com/generating-images-with-autoencoders-77fd3a8dd368"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YufGFkNU6G2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}